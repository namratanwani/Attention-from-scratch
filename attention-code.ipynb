{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism with custom input embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the word embeddings of only 3 tokens with their positions in input\n",
    "inp_emb1, pos1 = np.array([1,2,3,4]), 1\n",
    "inp_emb2, pos2 = np.array([4,5,2,3]), 2\n",
    "inp_emb3, pos3 = np.array([6,1,2,2]), 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \"Attention is all you ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dimension for model is 4, for simplicity.\n",
    "dim_model = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$PE(pos_{2i}) = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$PE(pos_{2i+1}) = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformed word embedding: $$y_1 \\cdot \\sqrt{d_{\\text{model}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model = 4\n",
    "def positional_encoding(inp_emb, pos):\n",
    "    \"\"\"\n",
    "    This function outputs the positional encoding of the inputs.\n",
    "    It first calculates the positional vector for each input using\n",
    "    sin and cos functions. Then, adds the positional vector to a\n",
    "    transformed word embedding.\n",
    "    The transformed word embedding retains the word embedding info\n",
    "    without being minimised by position information.\n",
    "    \"\"\"\n",
    "    pos_enc = [0]*dim_model\n",
    "\n",
    "    for i in range(0, dim_model-1, 2):\n",
    "        term = pos/(10000**((2*i)/dim_model))\n",
    "        pos_enc[i] = np.sin(term)\n",
    "        pos_enc[i+1] = np.cos(term)\n",
    "\n",
    "        transformed_word_emb = inp_emb*np.sqrt(dim_model)\n",
    "        pos_enc[i] = pos_enc[i] + transformed_word_emb[i]\n",
    "        pos_enc[i+1] = pos_enc[i+1] + transformed_word_emb[i+1]\n",
    "\n",
    "    \n",
    "    return pos_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc1 = positional_encoding(inp_emb1, pos1)\n",
    "pos_enc2 = positional_encoding(inp_emb2, pos2)\n",
    "pos_enc3 = positional_encoding(inp_emb3, pos3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.8414709848078967, 4.54030230586814, 6.000099999999834, 8.999999995]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.909297426825681, 9.583853163452858, 4.000199999998666, 6.99999998]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12.141120008059866, 1.0100075033995546, 4.0002999999955, 4.999999955000001]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_input = np.array([pos_enc1, pos_enc2, pos_enc3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.84147098,  4.54030231,  6.0001    ,  8.99999999],\n",
       "       [ 8.90929743,  9.58385316,  4.0002    ,  6.99999998],\n",
       "       [12.14112001,  1.0100075 ,  4.0003    ,  4.99999996]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside attention mechanism, each word vector has 3 representations: A query vector (Q), A key vector (K), and a value vector (V). Here,\n",
    "\n",
    "Q = attention_input x W_Q\n",
    "\n",
    "K = attention_input x W_K\n",
    "\n",
    "V = attention_input x W_V\n",
    "\n",
    "Q, K, and V are obtained from matrix multiplication of wight matricex and input matrix. Here, W_Q, W_K, W_V  are the weights for each vector learnt and updated during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, We use the initialsed values of these weight vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for future matrix multiplication the number of columns in first\n",
    "#  matrix should be equal to the number of rows of the second matrix.\n",
    "dim_model = 4\n",
    "np.random.seed(12)\n",
    "def initialise_weights(dim,n):\n",
    "    W_Q = np.random.randint(n, size = (dim, dim))\n",
    "    W_K= np.random.randint(n, size = (dim, dim))\n",
    "    W_V = np.random.randint(n, size = (dim, dim))\n",
    "    return W_Q, W_K, W_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q, W_K, W_V = initialise_weights(dim_model,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 1, 2],\n",
       "       [3, 3, 4, 0],\n",
       "       [1, 4, 1, 2],\n",
       "       [3, 2, 0, 0]])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 2, 1, 3],\n",
       "       [4, 3, 1, 0],\n",
       "       [2, 2, 0, 4],\n",
       "       [3, 1, 0, 0]])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 1, 3],\n",
       "       [0, 1, 1, 0],\n",
       "       [4, 0, 4, 1],\n",
       "       [3, 4, 3, 3]])"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.matmul(attention_input, W_Q)\n",
    "K = np.matmul(attention_input, W_K)\n",
    "V = np.matmul(attention_input, W_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55.14541986, 64.14571986, 27.00278021, 17.68314197],\n",
       "       [80.47965171, 85.48025173, 51.24491008, 25.81899485],\n",
       "       [58.4536824 , 65.45458244, 20.18145002, 32.28284002]])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 68.52729315,  40.30404888,   7.38177329,  32.52481295],\n",
       "       [102.9730023 ,  61.57055432,  18.49315059,  42.72869228],\n",
       "       [ 75.60510991,  40.31286248,  13.15112751,  52.42456002]])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[59.52481294, 49.06471524, 58.38217328, 41.52451294],\n",
       "       [63.72869222, 64.31174536, 55.49395053, 51.72809222],\n",
       "       [67.42455989, 57.43336735, 44.15232738, 55.42365989]])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q,K,V, dim):\n",
    "    Q_againt_Kvalues = np.matmul(Q, K.T) / np.sqrt(dim)\n",
    "    softmax_score = softmax(Q_againt_Kvalues)\n",
    "    final_attention = np.matmul(softmax_score, V)\n",
    "    return final_attention\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [63.72869222, 64.31174536, 55.49395053, 51.72809222],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here, dim_k = dim_model\n",
    "attention(Q,K,V, dim_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention mechanism using Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The big cat sat on the mat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'big', 'cat', 'sat', 'on', 'the', 'mat']"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The big cat sat on the mat\"\n",
    "data = sentence.lower().split()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using CBOW \n",
    "model = Word2Vec([data], min_count=1, vector_size=224, window=2)\n",
    "\n",
    "embedding_dict = {}\n",
    "for word in data:\n",
    "    embedding_dict[word] = model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = np.array([\n",
    "    embedding_dict[word] for word in data\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 224)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model = 224\n",
    "# pe = []*input_embeddings.shape[0]\n",
    "pe = [positional_encoding(input_embeddings[i], i+1) for i in range(input_embeddings.shape[0])]\n",
    "pe = np.array(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 224)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to implement multi-head attention. We "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
